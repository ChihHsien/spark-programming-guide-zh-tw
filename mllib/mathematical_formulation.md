# 數學公式(Mathematical formulation)
許多標準機器學習方法可以被轉換為凸優化問題(convex optimization problem), 即一個找到凸函數$$f$$最小值的任務，這個函數$$f$$依賴於一個有d個值的向量變量$$w$$(代碼中的weights)。更正式點，這是一個$$min_{w \in R^d} f(x)$$優化問題，其目標函數$$f$$具有下面形式：
$$
f(x):=\lambda R(w)+\frac{1}n \sum_{i=1}^{n}L(w; x_i, y_i)
$$
向量$$x_i \in R^d$$是訓練數據樣本，其中$$1\leq i\leq n$$。 $$y_i \in R$$是相對應的類標簽，也是我們想要預測的目標。如果$$L(w; x, y)$$能被表述為$$w^Tx$$和$$y$$的一個函數，我們稱該方法為線性的，有機個MLlib分類和迴歸算法屬於該范疇，我們在此一一討論。

目標函數$$f$$包括兩部份：控制模型複雜度的正則化因子和度量模型誤差的損失函數。損失函數$$L(w;.)$$是典型與$$w$$相關的凸函數。事先鎖定正則化參數$$\lambda \geq 0$$(代碼中的regParam)承載了我們在最小化損失量(訓練誤差)和最小化模型複雜度(避免過渡擬合)兩個目標之間的權衡取捨。




